{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and download the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook is running at: /home/gfair/pnnl-xai/Code/GabrielFair/SST_LSTM\n",
      "['/usr/lib/python35.zip', '/usr/lib/python3.5', '/usr/lib/python3.5/plat-x86_64-linux-gnu', '/usr/lib/python3.5/lib-dynload', '', '/usr/local/lib/python3.5/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.5/dist-packages/IPython/extensions', '/home/gfair/.ipython']\n",
      "3.5.2 (default, Nov 12 2018, 13:43:14) \n",
      "[GCC 5.4.0 20160609]\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import urllib.request as req\n",
    "import certifi\n",
    "try:\n",
    "    import pycurl\n",
    "except ImportError:\n",
    "    raise\n",
    "    print(\"pycurl is not installed or is not found by python\")\n",
    "    exit()\n",
    "\n",
    "if not os.path.exists(\"input_data\"):\n",
    "    os.makedirs(\"input_data\")\n",
    "\n",
    "glove_data_dir = \"glove_data\"\n",
    "if not os.path.exists(glove_data_dir):\n",
    "    os.makedirs(glove_data_dir)\n",
    "\n",
    "if os.path.exists(\"data\"):\n",
    "    print(\"Deleted previous output data\")\n",
    "    !rm -rf data\n",
    "    \n",
    "if not os.path.isfile(\"input_data/stanfordSentimentTreebank.zip\"):\n",
    "    with open(\"input_data/stanfordSentimentTreebank.zip\", 'wb') as f:\n",
    "        c = pycurl.Curl() \n",
    "        c.setopt(c.URL, \"https://nlp.stanford.edu/~socherr/stanfordSentimentTreebank.zip\")\n",
    "        c.setopt(c.WRITEDATA, f)\n",
    "        c.setopt(c.CAINFO, certifi.where())\n",
    "        c.perform()\n",
    "        c.close()\n",
    "    \n",
    "if not os.path.isfile(\"input_data/stanfordSentimentTreebankRaw.zip\"):\n",
    "    with open(\"input_data/stanfordSentimentTreebankRaw.zip\", 'wb') as j:\n",
    "        c = pycurl.Curl()\n",
    "        c.setopt(c.URL, \"https://nlp.stanford.edu/~socherr/stanfordSentimentTreebankRaw.zip\")\n",
    "        c.setopt(c.WRITEDATA, j)\n",
    "        c.setopt(c.CAINFO, certifi.where())\n",
    "        c.perform()\n",
    "        c.close()\n",
    "        \n",
    "if not os.path.isfile(\"input_data/trainDevTestTrees_PTB.zip\"):\n",
    "    with open(\"input_data/trainDevTestTrees_PTB.zip\", 'wb') as h:\n",
    "        c = pycurl.Curl()\n",
    "        c.setopt(c.URL, \"https://nlp.stanford.edu/sentiment/trainDevTestTrees_PTB.zip\")\n",
    "        c.setopt(c.WRITEDATA, h)\n",
    "        c.setopt(c.CAINFO, certifi.where())\n",
    "        c.perform()\n",
    "        c.close()\n",
    "        \n",
    "if not os.path.isfile(\"input_data/glove.840B.300d.zip\"):\n",
    "    with open(\"input_data/glove.840B.300d.zip\", 'wb') as h:\n",
    "        c = pycurl.Curl()\n",
    "        c.setopt(c.URL, \"https://nlp.stanford.edu/data/glove.840B.300d.zip\")\n",
    "        c.setopt(c.WRITEDATA, h)\n",
    "        c.setopt(c.CAINFO, certifi.where())\n",
    "        c.perform()\n",
    "        c.close()\n",
    "        \n",
    "        \n",
    "import zipfile\n",
    "with zipfile.ZipFile(\"input_data/stanfordSentimentTreebank.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"input_data/\")\n",
    "zip_ref.close()\n",
    "with zipfile.ZipFile(\"input_data/stanfordSentimentTreebankRaw.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"input_data/\")\n",
    "zip_ref.close()\n",
    "with zipfile.ZipFile(\"input_data/trainDevTestTrees_PTB.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"input_data/\")\n",
    "zip_ref.close()\n",
    "if not os.path.isfile(glove_data_dir + \"/glove.840B.300d.txt\"):\n",
    "    with zipfile.ZipFile(\"input_data/glove.840B.300d.zip\", 'r') as zip_ref:\n",
    "        zip_ref.extractall(glove_data_dir)\n",
    "    zip_ref.close()\n",
    "\n",
    "print(\"Notebook is running at: \"+ str(os.getcwd()))\n",
    "print(sys.path)\n",
    "import platform;\n",
    "\n",
    "print(platform.sys.version);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lasagne is not installed or is not found by python\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import numpy as np\n",
    "except ImportError:\n",
    "    print(\"numpy is not installed or is not found by python\")\n",
    "\n",
    "# try:\n",
    "#     from theano import tensor as T\n",
    "#     from theano import config\n",
    "# except ImportError:\n",
    "#     print(\"theano is not installed or is not found by python\")\n",
    "    \n",
    "try:\n",
    "    import tensorflow as tf\n",
    "#     sess = tf.InteractiveSession()\n",
    "except ImportError:\n",
    "    print(\"tensorflow is not installed or is not found by python\")\n",
    "    \n",
    "#try:\n",
    "#    import tensorflow_fold as td\n",
    "#except ImportError:\n",
    "#    print(\"tensorflow_fold is not installed or is not found by python:\")\n",
    "#    print(\"https://github.com/tensorflow/fold/blob/master/tensorflow_fold/g3doc/setup.md\")\n",
    "\n",
    "try:\n",
    "    import lasagne\n",
    "except ImportError:\n",
    "    print(\"lasagne is not installed or is not found by python\")\n",
    "\n",
    "import pickle\n",
    "import codecs\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "except ImportError:\n",
    "    print(\"torch is not installed or is not found by python\")\n",
    "    \n",
    "try:\n",
    "    import sklearn\n",
    "except ImportError:\n",
    "    print(\"sklearn is not installed or is not found by python\")\n",
    "    \n",
    "try:\n",
    "    import scipy\n",
    "except ImportError:\n",
    "    print(\"scipy is not installed or is not found by python\")\n",
    "    \n",
    "try:\n",
    "    from nltk.tokenize import sexpr\n",
    "except ImportError:\n",
    "    print(\"nltk is not installed or is not found by python\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 8544 trees from input_data/trees/train.txt\n",
      "loaded 1101 trees from input_data/trees/dev.txt\n",
      "loaded 2210 trees from input_data/trees/test.txt\n"
     ]
    }
   ],
   "source": [
    "def load_trees(filename):\n",
    "    with codecs.open(filename, encoding='utf-8') as f:\n",
    "        # Drop the trailing newline and strip \\s.\n",
    "        trees = [line.strip().replace('\\\\', '') for line in f]\n",
    "        print('loaded %s trees from %s' % (len(trees), filename))\n",
    "        return trees\n",
    "\n",
    "\n",
    "vocab = set()\n",
    "with codecs.open(\"input_data/stanfordSentimentTreebank/SOStr.txt\", encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        # Drop the trailing newline and strip backslashes. Split into words.\n",
    "        vocab.update(line.strip().replace('\\\\', '').split('|'))\n",
    "\n",
    "def check_if_word_embeddings_exist():\n",
    "    if not os.path.isfile(glove_data_dir + '/filtered_glove.txt'):\n",
    "        print('Saving glove files to %s' % glove_data_dir)\n",
    "        nread = 0\n",
    "        nwrote = 0\n",
    "        with codecs.open(glove_data_dir + \"/glove.840B.300d.txt\", encoding='utf-8') as f:\n",
    "            with codecs.open(glove_data_dir + '/filtered_glove.txt', 'w', encoding='utf-8') as out:\n",
    "                for line in f:\n",
    "                    nread += 1\n",
    "                    line = line.strip()\n",
    "                    if not line: continue\n",
    "                    if line.split(u' ', 1)[0] in vocab:\n",
    "                        out.write(line + '\\n')\n",
    "                        nwrote += 1\n",
    "        print('read %s lines, wrote %s' % (nread, nwrote))\n",
    "    return\n",
    "\n",
    "\n",
    "check_if_word_embeddings_exist()\n",
    "train_trees = load_trees(\"input_data/trees/train.txt\")\n",
    "dev_trees = load_trees(\"input_data/trees/dev.txt\")\n",
    "test_trees = load_trees(\"input_data/trees/test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities to Tokenize and load the Stanford corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word embeddings from glove_data/filtered_glove.txt\n"
     ]
    }
   ],
   "source": [
    "def tokenize(s):\n",
    "    label, phrase = s[1:-1].split(None, 1)\n",
    "    return label, sexpr.sexpr_tokenize(phrase)\n",
    "\n",
    "def load_embeddings(embedding_path):\n",
    "    \"\"\"Loads embedings, returns weight matrix and dict from words to indices.\"\"\"\n",
    "    print('loading word embeddings from %s' % embedding_path)\n",
    "    weight_vectors = []\n",
    "    word_idx = {}\n",
    "    with codecs.open(embedding_path, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            word, vec = line.split(u' ', 1)\n",
    "            word_idx[word] = len(weight_vectors)\n",
    "            weight_vectors.append(np.array(vec.split(), dtype=np.float32))\n",
    "    # Annoying implementation detail; '(' and ')' are replaced by '-LRB-' and\n",
    "    # '-RRB-' respectively in the parse-trees.\n",
    "    word_idx[u'-LRB-'] = word_idx.pop(u'(')\n",
    "    word_idx[u'-RRB-'] = word_idx.pop(u')')\n",
    "    # Random embedding vector for unknown words.\n",
    "    weight_vectors.append(np.random.uniform(\n",
    "        -0.05, 0.05, weight_vectors[0].shape).astype(np.float32))\n",
    "    return np.stack(weight_vectors), word_idx\n",
    "\n",
    "weight_matrix, word_idx = load_embeddings(glove_data_dir + '/filtered_glove.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class to hold the LSTM Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeLSTMModel(object):\n",
    "    def __init__(self, weight_matrix, word_idx, ModelConfig):\n",
    "\n",
    "        self.ModelConfig = ModelConfig\n",
    "\n",
    "        self.word_embedding = td.Embedding(*weight_matrix.shape, initializer=weight_matrix, name='word_embedding')\n",
    "        self.word_idx = word_idx\n",
    "\n",
    "        self.keep_prob_ph = tf.placeholder_with_default(1.0, [])\n",
    "        self.tree_lstm = td.ScopedLayer(\n",
    "            tf.contrib.rnn.DropoutWrapper(\n",
    "                BinaryTreeLSTMCell(self.ModelConfig.lstm_num_units, keep_prob=self.keep_prob_ph),\n",
    "                input_keep_prob=self.keep_prob_ph, output_keep_prob=self.keep_prob_ph),\n",
    "            name_or_scope='tree_lstm')\n",
    "        self.output_layer = td.FC(self.ModelConfig.num_classes, activation=None, name='output_layer')\n",
    "\n",
    "        self.embed_subtree = td.ForwardDeclaration(name='embed_subtree')\n",
    "        self.model = self.embed_tree(is_root=True)\n",
    "        self.embed_subtree.resolve_to(self.embed_tree(is_root=False))\n",
    "\n",
    "        self.compiler = td.Compiler.create(self.model)\n",
    "        print('input type: %s' % self.model.input_type)\n",
    "        print('output type: %s' % self.model.output_type)\n",
    "\n",
    "        self.metrics = {k: tf.reduce_mean(v) for k, v in self.compiler.metric_tensors.items()}\n",
    "\n",
    "        self.loss = tf.reduce_sum(self.compiler.metric_tensors['all_loss'])\n",
    "        opt = tf.train.AdagradOptimizer(ModelConfig.learning_rate)\n",
    "\n",
    "        grads_and_vars = opt.compute_gradients(self.loss)\n",
    "        found = 0\n",
    "        for i, (grad, var) in enumerate(grads_and_vars):\n",
    "            if var == self.word_embedding.weights:\n",
    "                found += 1\n",
    "                grad = tf.scalar_mul(ModelConfig.embedding_learning_rate, grad)\n",
    "                grads_and_vars[i] = (grad, var)\n",
    "        assert found == 1  # internal consistency check\n",
    "        self.train_op = opt.apply_gradients(grads_and_vars)\n",
    "\n",
    "        self.saver = tf.train.Saver()\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def train_epoch(self, train_set, batch_size):\n",
    "        loss = 0\n",
    "        for batch in td.group_by_batches(train_set, batch_size):\n",
    "            train_feed_dict = {self.keep_prob_ph: self.ModelConfig.keep_prob, self.compiler.loom_input_tensor: batch}\n",
    "            loss += self.train_step(train_feed_dict)\n",
    "        return loss\n",
    "\n",
    "    def train_step(self, train_feed_dict):\n",
    "        _, batch_loss = self.sess.run([self.train_op, self.loss], train_feed_dict)\n",
    "        return batch_loss\n",
    "\n",
    "    def dev_eval(self, epoch, train_loss, dev_feed_dict):\n",
    "        dev_metrics = self.sess.run(self.metrics, dev_feed_dict)\n",
    "        dev_loss = dev_metrics['all_loss']\n",
    "        dev_accuracy = ['%s: %.2f' % (k, v * 100) for k, v in\n",
    "                        sorted(dev_metrics.items()) if k.endswith('hits')]\n",
    "        print('epoch:%4d, train_loss: %.3e, dev_loss_avg: %.3e, dev_accuracy:\\n  [%s]'\n",
    "              % (epoch, train_loss, dev_loss, ' '.join(dev_accuracy)))\n",
    "        return dev_metrics['root_hits']\n",
    "\n",
    "    def embed_tree(self, is_root):\n",
    "        \"\"\"Creates a block that embeds trees; output is tree LSTM state.\"\"\"\n",
    "        return td.InputTransform(tokenize) >> td.OneOf(\n",
    "            key_fn=lambda pair: pair[0] == '2',  # label 2 means neutral\n",
    "            case_blocks=(self.add_metrics(is_root, is_neutral=False),\n",
    "                         self.add_metrics(is_root, is_neutral=True)),\n",
    "            pre_block=(td.Scalar('int32'), self.logits_and_state()))\n",
    "\n",
    "    def logits_and_state(self):\n",
    "        \"\"\"Creates a block that goes from tokens to (logits, state) tuples.\"\"\"\n",
    "        unknown_idx = len(self.word_idx)\n",
    "        lookup_word = lambda word: self.word_idx.get(word, unknown_idx)\n",
    "\n",
    "        word2vec = (td.GetItem(0) >> td.InputTransform(lookup_word) >>\n",
    "                    td.Scalar('int32') >> self.word_embedding)\n",
    "\n",
    "        pair2vec = (self.embed_subtree(), self.embed_subtree())\n",
    "\n",
    "        # Trees are binary, so the tree layer takes two states as its input_state.\n",
    "        zero_state = td.Zeros((self.tree_lstm.state_size,) * 2)\n",
    "        # Input is a word vector.\n",
    "        zero_inp = td.Zeros(self.word_embedding.output_type.shape[0])\n",
    "\n",
    "        word_case = td.AllOf(word2vec, zero_state)\n",
    "        pair_case = td.AllOf(zero_inp, pair2vec)\n",
    "\n",
    "        tree2vec = td.OneOf(len, [(1, word_case), (2, pair_case)])\n",
    "\n",
    "        return tree2vec >> self.tree_lstm >> (self.output_layer, td.Identity())\n",
    "\n",
    "    def add_metrics(self, is_root, is_neutral):\n",
    "        \"\"\"A block that adds metrics for loss and hits; output is the LSTM state.\"\"\"\n",
    "        c = td.Composition(\n",
    "            name='predict(is_root=%s, is_neutral=%s)' % (is_root, is_neutral))\n",
    "        with c.scope():\n",
    "            # destructure the input; (labels, (logits, state))\n",
    "            labels = c.input[0]\n",
    "            logits = td.GetItem(0).reads(c.input[1])\n",
    "            state = td.GetItem(1).reads(c.input[1])\n",
    "\n",
    "            # calculate loss\n",
    "            loss = td.Function(self.tf_node_loss)\n",
    "            td.Metric('all_loss').reads(loss.reads(logits, labels))\n",
    "            if is_root:\n",
    "                td.Metric('root_loss').reads(loss)\n",
    "\n",
    "            # calculate fine-grained hits\n",
    "            hits = td.Function(self.tf_fine_grained_hits)\n",
    "            td.Metric('all_hits').reads(hits.reads(logits, labels))\n",
    "            if is_root:\n",
    "                td.Metric('root_hits').reads(hits)\n",
    "\n",
    "            # calculate binary hits, if the label is not neutral\n",
    "            if not is_neutral:\n",
    "                binary_hits = td.Function(self.tf_binary_hits).reads(logits, labels)\n",
    "                td.Metric('all_binary_hits').reads(binary_hits)\n",
    "                if is_root:\n",
    "                    td.Metric('root_binary_hits').reads(binary_hits)\n",
    "\n",
    "            # output the state, which will be read by our by parent's LSTM cell\n",
    "            c.output.reads(state)\n",
    "        return c\n",
    "\n",
    "    @staticmethod\n",
    "    def tf_node_loss(logits, labels):\n",
    "        return tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n",
    "\n",
    "    @staticmethod\n",
    "    def tf_fine_grained_hits(logits, labels):\n",
    "        predictions = tf.cast(tf.argmax(logits, 1), tf.int32)\n",
    "        return tf.cast(tf.equal(predictions, labels), tf.float64)\n",
    "\n",
    "    @staticmethod\n",
    "    def tf_binary_hits(logits, labels):\n",
    "        softmax = tf.nn.softmax(logits)\n",
    "        binary_predictions = (softmax[:, 3] + softmax[:, 4]) > (softmax[:, 0] + softmax[:, 1])\n",
    "        binary_labels = labels > 2\n",
    "        return tf.cast(tf.equal(binary_predictions, binary_labels), tf.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the LSTM model after tokenizing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'td' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-72f3c1e4ab92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0membedding_learning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtree_lstm_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTreeLSTMModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModelConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mtrain_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_lstm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_loom_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_trees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-7e32d2362c78>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, weight_matrix, word_idx, ModelConfig)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModelConfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mweight_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'word_embedding'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'td' is not defined"
     ]
    }
   ],
   "source": [
    "# Tai et al. https://arxiv.org/pdf/1503.00075.pdf\n",
    "\n",
    "class ModelConfig(object):\n",
    "    num_classes = 5\n",
    "    lstm_num_units = 300  # Tai et al. used 150, but our regularization strategy is more effective\n",
    "    learning_rate = 0.05\n",
    "    keep_prob = 0.75\n",
    "    batch_size = 100\n",
    "    epochs = 20\n",
    "    embedding_learning_rate = 0.1\n",
    "    \n",
    "tree_lstm_model = TreeLSTMModel(weight_matrix, word_idx, ModelConfig)\n",
    "train_set = tree_lstm_model.compiler.build_loom_inputs(train_trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_feed_dict = tree_lstm_model.compiler.build_feed_dict(dev_trees)\n",
    "\n",
    "best_accuracy = 0.0\n",
    "save_path = os.path.join(PathConfig.data_path, 'sentiment_model')\n",
    "for epoch, shuffled in enumerate(td.epochs(train_set, ModelConfig.epochs), 1):\n",
    "    train_loss = tree_lstm_model.train_epoch(shuffled, ModelConfig.batch_size)\n",
    "    accuracy = tree_lstm_model.dev_eval(epoch, train_loss, dev_feed_dict)\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        checkpoint_path = tree_lstm_model.saver.save(tree_lstm_model.sess, save_path, global_step=epoch)\n",
    "        print('model saved in file: %s' % checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
